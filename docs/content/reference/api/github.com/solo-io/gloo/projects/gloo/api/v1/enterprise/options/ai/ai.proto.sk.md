
---
title: "ai.proto"
weight: 5
---

<!-- Code generated by solo-kit. DO NOT EDIT. -->


### Package: `ai.options.gloo.solo.io` 
#### Types:


- [SingleAuthToken](#singleauthtoken)
- [Passthrough](#passthrough)
- [UpstreamSpec](#upstreamspec)
- [CustomHost](#customhost)
- [OpenAI](#openai)
- [AzureOpenAI](#azureopenai)
- [Gemini](#gemini)
- [VertexAI](#vertexai)
- [Publisher](#publisher)
- [Mistral](#mistral)
- [Anthropic](#anthropic)
- [MultiPool](#multipool)
- [Backend](#backend)
- [Priority](#priority)
- [RouteSettings](#routesettings)
- [RouteType](#routetype)
- [FieldDefault](#fielddefault)
- [Postgres](#postgres)
- [Embedding](#embedding)
- [OpenAI](#openai)
- [AzureOpenAI](#azureopenai)
- [SemanticCache](#semanticcache)
- [Redis](#redis)
- [Weaviate](#weaviate)
- [DataStore](#datastore)
- [Mode](#mode)
- [RAG](#rag)
- [DataStore](#datastore)
- [AIPromptEnrichment](#aipromptenrichment)
- [Message](#message)
- [AIPromptGuard](#aipromptguard)
- [Regex](#regex)
- [RegexMatch](#regexmatch)
- [BuiltIn](#builtin)
- [Action](#action)
- [Webhook](#webhook)
- [HeaderMatch](#headermatch)
- [MatchType](#matchtype)
- [Moderation](#moderation)
- [OpenAI](#openai)
- [Request](#request)
- [CustomResponse](#customresponse)
- [Response](#response)
  



##### Source File: [github.com/solo-io/gloo/projects/controller/api/v1/enterprise/options/ai/ai.proto](https://github.com/solo-io/gloo/blob/main/projects/controller/api/v1/enterprise/options/ai/ai.proto)





---
### SingleAuthToken



```yaml
"inline": string
"secretRef": .core.solo.io.ResourceRef
"passthrough": .ai.options.gloo.solo.io.SingleAuthToken.Passthrough

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `inline` | `string` | Provide easy inline way to specify a token. Only one of `inline`, `secretRef`, or `passthrough` can be set. |
| `secretRef` | [.core.solo.io.ResourceRef](../../../../../../../../../solo-kit/api/v1/ref.proto.sk/#resourceref) | Reference to a secret in the same namespace as the Upstream. Only one of `secretRef`, `inline`, or `passthrough` can be set. |
| `passthrough` | [.ai.options.gloo.solo.io.SingleAuthToken.Passthrough](../ai.proto.sk/#passthrough) | Passthrough the existing token. This token can either come directly from the client, or be generated by an OIDC flow early in the request lifecycle. This option is useful for backends which have federated identity setup and can re-use the token from the client. Currently this token must exist in the `Authorization` header. Only one of `passthrough`, `inline`, or `secretRef` can be set. |




---
### Passthrough



```yaml

```

| Field | Type | Description |
| ----- | ---- | ----------- | 




---
### UpstreamSpec

 
The AI UpstreamSpec represents a logical LLM provider backend.
The purpose of this spec is a way to configure which backend to use
as well as how to authenticate with the backend.

Currently the options are:
- OpenAI
Default Host: api.openai.com
Default Port: 443
Auth Token: Bearer token to use for the OpenAI API
- Mistral
Default Host: api.mistral.com
Default Port: 443
Auth Token: Bearer token to use for the Mistral API
- Anthropic
Default Host: api.anthropic.com
Default Port: 443
Auth Token: x-api-key to use for the Anthropic API
Version: Optional version header to pass to the Anthropic API

All of the above backends can be configured to use a custom host and port.
This option is meant to allow users to proxy the request, or to use a different
backend altogether which is API compliant with the upstream version.

Examples:

OpenAI with inline auth token:
```
ai:
openai:
authToken:
inline: "my_token"
```

Mistral with secret ref:
```
ai:
mistral:
authToken:
secretRef:
name: "my-secret"
namespace: "my-ns"
```

Anthropic with inline token and custom Host:
```
ai:
anthropic:
authToken:
inline: "my_token"
customHost:
host: "my-anthropic-host.com"
port: 443 # Port is optional and will default to 443 for HTTPS
```

```yaml
"openai": .ai.options.gloo.solo.io.UpstreamSpec.OpenAI
"mistral": .ai.options.gloo.solo.io.UpstreamSpec.Mistral
"anthropic": .ai.options.gloo.solo.io.UpstreamSpec.Anthropic
"azureOpenai": .ai.options.gloo.solo.io.UpstreamSpec.AzureOpenAI
"multi": .ai.options.gloo.solo.io.UpstreamSpec.MultiPool
"gemini": .ai.options.gloo.solo.io.UpstreamSpec.Gemini
"vertexAi": .ai.options.gloo.solo.io.UpstreamSpec.VertexAI

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `openai` | [.ai.options.gloo.solo.io.UpstreamSpec.OpenAI](../ai.proto.sk/#openai) | OpenAI upstream. Only one of `openai`, `mistral`, `anthropic`, `azureOpenai`, `multi`, `gemini`, or `vertexAi` can be set. |
| `mistral` | [.ai.options.gloo.solo.io.UpstreamSpec.Mistral](../ai.proto.sk/#mistral) | Mistral upstream. Only one of `mistral`, `openai`, `anthropic`, `azureOpenai`, `multi`, `gemini`, or `vertexAi` can be set. |
| `anthropic` | [.ai.options.gloo.solo.io.UpstreamSpec.Anthropic](../ai.proto.sk/#anthropic) | Anthropic upstream. Only one of `anthropic`, `openai`, `mistral`, `azureOpenai`, `multi`, `gemini`, or `vertexAi` can be set. |
| `azureOpenai` | [.ai.options.gloo.solo.io.UpstreamSpec.AzureOpenAI](../ai.proto.sk/#azureopenai) | Azure OpenAI upstream. Only one of `azureOpenai`, `openai`, `mistral`, `anthropic`, `multi`, `gemini`, or `vertexAi` can be set. |
| `multi` | [.ai.options.gloo.solo.io.UpstreamSpec.MultiPool](../ai.proto.sk/#multipool) | multi upstream. Only one of `multi`, `openai`, `mistral`, `anthropic`, `azureOpenai`, `gemini`, or `vertexAi` can be set. |
| `gemini` | [.ai.options.gloo.solo.io.UpstreamSpec.Gemini](../ai.proto.sk/#gemini) | Gemini upstream. Only one of `gemini`, `openai`, `mistral`, `anthropic`, `azureOpenai`, `multi`, or `vertexAi` can be set. |
| `vertexAi` | [.ai.options.gloo.solo.io.UpstreamSpec.VertexAI](../ai.proto.sk/#vertexai) | Vertex AI upstream. Only one of `vertexAi`, `openai`, `mistral`, `anthropic`, `azureOpenai`, `multi`, or `gemini` can be set. |




---
### CustomHost

 
Settings to configure a custom host to send the traffic to

```yaml
"host": string
"port": int

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `host` | `string` | Custom host to send the traffic to. |
| `port` | `int` | Custom port to send the traffic to. |




---
### OpenAI

 
Settings for the OpenAI API

```yaml
"authToken": .ai.options.gloo.solo.io.SingleAuthToken
"customHost": .ai.options.gloo.solo.io.UpstreamSpec.CustomHost
"model": string

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `authToken` | [.ai.options.gloo.solo.io.SingleAuthToken](../ai.proto.sk/#singleauthtoken) | Auth Token to use for the OpenAI API This token will be placed into the `Authorization` header and prefixed with Bearer if not present when sending the request to the upstream. |
| `customHost` | [.ai.options.gloo.solo.io.UpstreamSpec.CustomHost](../ai.proto.sk/#customhost) | Optional custom host to send the traffic to. |
| `model` | `string` | Optional: override model name. If not set, the model name will be taken from the request This can be useful when trying model failover scenarios e.g. "gpt-4o-mini". |




---
### AzureOpenAI

 
Settings for the Azure OpenAI API

```yaml
"authToken": .ai.options.gloo.solo.io.SingleAuthToken
"endpoint": string
"deploymentName": string
"apiVersion": string

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `authToken` | [.ai.options.gloo.solo.io.SingleAuthToken](../ai.proto.sk/#singleauthtoken) | Auth Token to use for the OpenAI API This token will be placed into the `api-key` header. |
| `endpoint` | `string` | The endpoint to use This should be the endpoint to the Azure OpenAI API, e.g. my-endpoint.openai.azure.com If the scheme is included it will be stripped. This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}. |
| `deploymentName` | `string` | The deployment/model name to use This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}. |
| `apiVersion` | `string` | The version of the API to use This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}. |




---
### Gemini

 
Settings for the Gemini API

```yaml
"authToken": .ai.options.gloo.solo.io.SingleAuthToken
"model": string
"apiVersion": string

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `authToken` | [.ai.options.gloo.solo.io.SingleAuthToken](../ai.proto.sk/#singleauthtoken) | Auth Token to use for the Gemini API This token will be placed into the `key` header. |
| `model` | `string` | The model name to use This value can be found https://generativelanguage.googleapis.com/{version}/models/{model}:generateContent?key={api_key}. |
| `apiVersion` | `string` | The version of the API to use This value can be found https://generativelanguage.googleapis.com/{api_version}/models/{model}:generateContent?key={api_key}. |




---
### VertexAI

 
Settings for the Vertex AI API

```yaml
"authToken": .ai.options.gloo.solo.io.SingleAuthToken
"model": string
"apiVersion": string
"projectId": string
"location": string
"modelPath": string
"publisher": .ai.options.gloo.solo.io.UpstreamSpec.VertexAI.Publisher

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `authToken` | [.ai.options.gloo.solo.io.SingleAuthToken](../ai.proto.sk/#singleauthtoken) | Auth Token to use for the Vertex AI API This token will be placed into the `Authorization: Bearer ` header. |
| `model` | `string` | The model name to use This value can be found https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models. |
| `apiVersion` | `string` | The version of the API to use. See https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models for supported models for specific publishers. |
| `projectId` | `string` | Google Cloud Project ID in https://{LOCATION}-aiplatform.googleapis.com/{VERSION}/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/{PROVIDER}/<model-path>. |
| `location` | `string` | Location of the project in https://{LOCATION}-aiplatform.googleapis.com/{VERSION}/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/{PROVIDER}/<model-path>. |
| `modelPath` | `string` | Model path (optional). Defaults to Gemini model path (generateContent). |
| `publisher` | [.ai.options.gloo.solo.io.UpstreamSpec.VertexAI.Publisher](../ai.proto.sk/#publisher) | The type of model publishers to use, currently only Google is supported in https://{LOCATION}-aiplatform.googleapis.com/{VERSION}/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/{PUBLISHER}/<model-path>. |




---
### Publisher



| Name | Description |
| ----- | ----------- | 
| `GOOGLE` |  |




---
### Mistral

 
Settings for the Mistral API

```yaml
"authToken": .ai.options.gloo.solo.io.SingleAuthToken
"customHost": .ai.options.gloo.solo.io.UpstreamSpec.CustomHost
"model": string

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `authToken` | [.ai.options.gloo.solo.io.SingleAuthToken](../ai.proto.sk/#singleauthtoken) | Auth Token to use for the Mistral API. This token will be placed into the `Authorization` header and prefixed with Bearer if not present when sending the request to the upstream. |
| `customHost` | [.ai.options.gloo.solo.io.UpstreamSpec.CustomHost](../ai.proto.sk/#customhost) | Optional custom host to send the traffic to. |
| `model` | `string` | Optional: override model name. If not set, the model name will be taken from the request This can be useful when trying model failover scenarios. |




---
### Anthropic

 
Settings for the Anthropic API

```yaml
"authToken": .ai.options.gloo.solo.io.SingleAuthToken
"customHost": .ai.options.gloo.solo.io.UpstreamSpec.CustomHost
"version": string
"model": string

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `authToken` | [.ai.options.gloo.solo.io.SingleAuthToken](../ai.proto.sk/#singleauthtoken) | Auth Token to use for the Anthropic API. This token will be placed into the `x-api-key` header when sending the request to the upstream. |
| `customHost` | [.ai.options.gloo.solo.io.UpstreamSpec.CustomHost](../ai.proto.sk/#customhost) |  |
| `version` | `string` | An optional version header to pass to the Anthropic API See: https://docs.anthropic.com/en/api/versioning for more details. |
| `model` | `string` | Optional: override model name. If not set, the model name will be taken from the request This can be useful when trying model failover scenarios. |




---
### MultiPool

 
multi:
pools:
- pool:
- openai:
authToken:
secretRef:
name: openai-secret
namespace: gloo-system
priority: 1
- pool:
- azureOpenai:
deploymentName: gpt-4o-mini
apiVersion: 2024-02-15-preview
endpoint: ai-gateway.openai.azure.com
authToken:
secretRef:
name: azure-secret
namespace: gloo-system
- azureOpenai:
deploymentName: gpt-4o-mini-2
apiVersion: 2024-02-15-preview
endpoint: ai-gateway.openai.azure.com
authToken:
secretRef:
name: azure-secret
namespace: gloo-system
priority: 2

```yaml
"priorities": []ai.options.gloo.solo.io.UpstreamSpec.MultiPool.Priority

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `priorities` | [[]ai.options.gloo.solo.io.UpstreamSpec.MultiPool.Priority](../ai.proto.sk/#priority) | List of prioritized backend pools. |




---
### Backend



```yaml
"openai": .ai.options.gloo.solo.io.UpstreamSpec.OpenAI
"mistral": .ai.options.gloo.solo.io.UpstreamSpec.Mistral
"anthropic": .ai.options.gloo.solo.io.UpstreamSpec.Anthropic
"azureOpenai": .ai.options.gloo.solo.io.UpstreamSpec.AzureOpenAI
"gemini": .ai.options.gloo.solo.io.UpstreamSpec.Gemini
"vertexAi": .ai.options.gloo.solo.io.UpstreamSpec.VertexAI

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `openai` | [.ai.options.gloo.solo.io.UpstreamSpec.OpenAI](../ai.proto.sk/#openai) | OpenAI upstream. Only one of `openai`, `mistral`, `anthropic`, `azureOpenai`, `gemini`, or `vertexAi` can be set. |
| `mistral` | [.ai.options.gloo.solo.io.UpstreamSpec.Mistral](../ai.proto.sk/#mistral) | Mistral upstream. Only one of `mistral`, `openai`, `anthropic`, `azureOpenai`, `gemini`, or `vertexAi` can be set. |
| `anthropic` | [.ai.options.gloo.solo.io.UpstreamSpec.Anthropic](../ai.proto.sk/#anthropic) | Anthropic upstream. Only one of `anthropic`, `openai`, `mistral`, `azureOpenai`, `gemini`, or `vertexAi` can be set. |
| `azureOpenai` | [.ai.options.gloo.solo.io.UpstreamSpec.AzureOpenAI](../ai.proto.sk/#azureopenai) | Azure OpenAI upstream. Only one of `azureOpenai`, `openai`, `mistral`, `anthropic`, `gemini`, or `vertexAi` can be set. |
| `gemini` | [.ai.options.gloo.solo.io.UpstreamSpec.Gemini](../ai.proto.sk/#gemini) | Gemini upstream. Only one of `gemini`, `openai`, `mistral`, `anthropic`, `azureOpenai`, or `vertexAi` can be set. |
| `vertexAi` | [.ai.options.gloo.solo.io.UpstreamSpec.VertexAI](../ai.proto.sk/#vertexai) | Vertex AI upstream. Only one of `vertexAi`, `openai`, `mistral`, `anthropic`, `azureOpenai`, or `gemini` can be set. |




---
### Priority

 
Priority represents a single endpoint pool with a given priority

```yaml
"pool": []ai.options.gloo.solo.io.UpstreamSpec.MultiPool.Backend

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `pool` | [[]ai.options.gloo.solo.io.UpstreamSpec.MultiPool.Backend](../ai.proto.sk/#backend) | list of backends representing a single endpoint pool. |




---
### RouteSettings

 
RouteSettings is a way to configure the behavior of the LLM provider on a per-route basis
This allows users to configure things like:
- Prompt Enrichment
- Retrieval Augmented Generation
- Semantic Caching
- Defaults to merge with the user input fields
- Guardrails
- Route Type

NOTE: These settings may only be applied to a route which uses an LLMProvider backend!

```yaml
"promptEnrichment": .ai.options.gloo.solo.io.AIPromptEnrichment
"promptGuard": .ai.options.gloo.solo.io.AIPromptGuard
"rag": .ai.options.gloo.solo.io.RAG
"semanticCache": .ai.options.gloo.solo.io.SemanticCache
"defaults": []ai.options.gloo.solo.io.FieldDefault
"routeType": .ai.options.gloo.solo.io.RouteSettings.RouteType

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `promptEnrichment` | [.ai.options.gloo.solo.io.AIPromptEnrichment](../ai.proto.sk/#aipromptenrichment) | Config used to enrich the prompt. This can only be used with LLMProviders using the CHAT API type. Prompt enrichment allows you to add additional context to the prompt before sending it to the model. Unlike RAG or other dynamic context methods, prompt enrichment is static and will be applied to every request. Note: Some providers, including Anthropic do not support SYSTEM role messages, but rather have a dedicated system field in the input JSON. In this case, `field_defaults` should be used to set the system field. See the docs for that field for an example. Example: ``` promptEnrichment: prepend: - role: SYSTEM content: "answer all questions in french" append: - role: USER content: "Describe the painting as if you were a famous art critic from the 17th century" ```. |
| `promptGuard` | [.ai.options.gloo.solo.io.AIPromptGuard](../ai.proto.sk/#aipromptguard) | Guards to apply to the LLM requests on this route. This can be used to reject requests based on the content of the prompt, as well as mask responses based on the content of the response. These guards can be also be used at the same time. Below is a simple example of a prompt guard that will reject any prompt that contains the string "credit card" and will mask any credit card numbers in the response. ``` promptGuard: request: customResponseMessage: "Rejected due to inappropriate content" regex: matches: - "credit card" response: regex: matches: # Mastercard - '(?:^|\D)(5[1-5][0-9]{2}(?:\ |\-|)[0-9]{4}(?:\ |\-|)[0-9]{4}(?:\ |\-|)[0-9]{4})(?:\D|$)' ````. |
| `rag` | [.ai.options.gloo.solo.io.RAG](../ai.proto.sk/#rag) | Retrieval Augmented Generation. https://research.ibm.com/blog/retrieval-augmented-generation-RAG Retrieval Augmented Generation is a process by which you "augment" the information a model has access to by providing it with a set of documents to use as context. This can be used to improve the quality of the generated text. Important Note: The same embedding mechanism must be used for the prompt which was used for the initial creation of the context documents. Example using postgres for storage and OpenAI for embedding: ``` rag: datastore: postgres: connectionString: postgresql+psycopg://gloo:gloo@172.17.0.1:6024/gloo collectionName: default embedding: openai: authToken: secretRef: name: openai-secret namespace: gloo-system ```. |
| `semanticCache` | [.ai.options.gloo.solo.io.SemanticCache](../ai.proto.sk/#semanticcache) | Semantic caching configuration Semantic caching allows you to cache previous model responses in order to provide faster responses to similar requests in the future. Results will vary depending on the embedding mechanism used, as well as the similarity threshold set. Example using Redis for storage and OpenAI for embedding: ``` semanticCache: datastore: redis: connectionString: redis://172.17.0.1:6379 embedding: openai: authToken: secretRef: name: openai-secret namespace: gloo-system ```. |
| `defaults` | [[]ai.options.gloo.solo.io.FieldDefault](../ai.proto.sk/#fielddefault) | A list of defaults to be merged with the user input fields. These will NOT override the user input fields unless override is explicitly set to true. Some examples include setting the temperature, max_tokens, etc. Example overriding system field for Anthropic: ``` # Anthropic doesn't support a system chat type defaults: - field: "system" value: "answer all questions in french" ``` Example setting the temperature and max_tokens, overriding max_tokens: ``` defaults: - field: "temperature" value: 0.5 - field: "max_tokens" value: 100 ```. |
| `routeType` | [.ai.options.gloo.solo.io.RouteSettings.RouteType](../ai.proto.sk/#routetype) | The type of route this is, currently only CHAT and CHAT_STREAMING are supported. |




---
### RouteType



| Name | Description |
| ----- | ----------- | 
| `CHAT` |  |
| `CHAT_STREAMING` |  |




---
### FieldDefault



```yaml
"field": string
"value": .google.protobuf.Value
"override": bool

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `field` | `string` | Field name. |
| `value` | [.google.protobuf.Value](https://developers.google.com/protocol-buffers/docs/reference/csharp/class/google/protobuf/well-known-types/value) | Field Value, this can be any valid JSON value. |
| `override` | `bool` | Whether or not to override the field if it already exists. |




---
### Postgres



```yaml
"connectionString": string
"collectionName": string

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `connectionString` | `string` | Connection string to the Postgres database. |
| `collectionName` | `string` | Name of the table to use. |




---
### Embedding



```yaml
"openai": .ai.options.gloo.solo.io.Embedding.OpenAI
"azureOpenai": .ai.options.gloo.solo.io.Embedding.AzureOpenAI

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `openai` | [.ai.options.gloo.solo.io.Embedding.OpenAI](../ai.proto.sk/#openai) | OpenAI embedding. Only one of `openai` or `azureOpenai` can be set. |
| `azureOpenai` | [.ai.options.gloo.solo.io.Embedding.AzureOpenAI](../ai.proto.sk/#azureopenai) | Azure OpenAI embedding. Only one of `azureOpenai` or `openai` can be set. |




---
### OpenAI

 
OpenAI embedding

```yaml
"authToken": .ai.options.gloo.solo.io.SingleAuthToken

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `authToken` | [.ai.options.gloo.solo.io.SingleAuthToken](../ai.proto.sk/#singleauthtoken) |  |




---
### AzureOpenAI

 
Azure OpenAI embedding

```yaml
"authToken": .ai.options.gloo.solo.io.SingleAuthToken
"apiVersion": string
"endpoint": string
"deploymentName": string

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `authToken` | [.ai.options.gloo.solo.io.SingleAuthToken](../ai.proto.sk/#singleauthtoken) | Auth Token to use for the OpenAI API This token will be placed into the `api-key` header. |
| `apiVersion` | `string` | The version of the API to use This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}. |
| `endpoint` | `string` | The endpoint to use This should be the endpoint to the Azure OpenAI API, e.g. https://my-endpoint.openai.azure.com If the scheme isn't included it will be added. This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}. |
| `deploymentName` | `string` | The deployment/model name to use This value can be found https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}. |




---
### SemanticCache

 
Settings for the Semantic Caching feature

```yaml
"datastore": .ai.options.gloo.solo.io.SemanticCache.DataStore
"embedding": .ai.options.gloo.solo.io.Embedding
"ttl": int
"mode": .ai.options.gloo.solo.io.SemanticCache.Mode

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `datastore` | [.ai.options.gloo.solo.io.SemanticCache.DataStore](../ai.proto.sk/#datastore) | Which data store to use. |
| `embedding` | [.ai.options.gloo.solo.io.Embedding](../ai.proto.sk/#embedding) | Model to use to get embeddings for prompt. |
| `ttl` | `int` | Time before data in the cache is considered expired. |
| `mode` | [.ai.options.gloo.solo.io.SemanticCache.Mode](../ai.proto.sk/#mode) | Cache mode to use: READ_WRITE or READ_ONLY. |




---
### Redis

 
Settings for the Redis database

```yaml
"connectionString": string
"scoreThreshold": float

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `connectionString` | `string` | Connection string to the Redis database. |
| `scoreThreshold` | `float` | Similarity score threshold value between 0.0 and 1.0 that determines how similar two queries need to be in order to return a cached result. The lower the number, the more similar the queries need to be for a cache hit. +kubebuilder:validation:Minimum=0 +kubebuilder:validation:Maximum=1. |




---
### Weaviate

 
Settings for the Weaviate database

```yaml
"host": string
"httpPort": int
"grpcPort": int
"insecure": bool

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `host` | `string` | Connection string to the Weaviate database, scheme should NOT be included. For example: weaviate.my-ns.svc.cluster.local NOT: http://weaviate.my-ns.svc.cluster.local. |
| `httpPort` | `int` | HTTP port to use, if unset will default to 8080. |
| `grpcPort` | `int` | GRPC port to use, if unset will default to 50051. |
| `insecure` | `bool` | Whether or not to use a secure connection, true by default. |




---
### DataStore

 
Data store from which to cache the request/response pairs

```yaml
"redis": .ai.options.gloo.solo.io.SemanticCache.Redis
"weaviate": .ai.options.gloo.solo.io.SemanticCache.Weaviate

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `redis` | [.ai.options.gloo.solo.io.SemanticCache.Redis](../ai.proto.sk/#redis) |  Only one of `redis` or `weaviate` can be set. |
| `weaviate` | [.ai.options.gloo.solo.io.SemanticCache.Weaviate](../ai.proto.sk/#weaviate) |  Only one of `weaviate` or `redis` can be set. |




---
### Mode



| Name | Description |
| ----- | ----------- | 
| `READ_WRITE` | Read and write to the cache as a part of the request/response lifecycle |
| `READ_ONLY` | Only read from the cache, do not write to it. Data will be written to the cache outside the request/response cycle. |




---
### RAG

 
Settings for the Retrieval Augmented Generation feature

```yaml
"datastore": .ai.options.gloo.solo.io.RAG.DataStore
"embedding": .ai.options.gloo.solo.io.Embedding
"promptTemplate": string

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `datastore` | [.ai.options.gloo.solo.io.RAG.DataStore](../ai.proto.sk/#datastore) | Data store from which to fetch the embeddings. |
| `embedding` | [.ai.options.gloo.solo.io.Embedding](../ai.proto.sk/#embedding) | Model to use to get embeddings for prompt. |
| `promptTemplate` | `string` | Template to use to embed the returned context. |




---
### DataStore



```yaml
"postgres": .ai.options.gloo.solo.io.Postgres

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `postgres` | [.ai.options.gloo.solo.io.Postgres](../ai.proto.sk/#postgres) |  |




---
### AIPromptEnrichment

 
Settings for the Prompt Enrichment feature

```yaml
"prepend": []ai.options.gloo.solo.io.AIPromptEnrichment.Message
"append": []ai.options.gloo.solo.io.AIPromptEnrichment.Message

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `prepend` | [[]ai.options.gloo.solo.io.AIPromptEnrichment.Message](../ai.proto.sk/#message) | A list of messages to be prepended to the prompt sent by the client. |
| `append` | [[]ai.options.gloo.solo.io.AIPromptEnrichment.Message](../ai.proto.sk/#message) | A list of messages to be appended to the prompt sent by the client. |




---
### Message



```yaml
"role": string
"content": string

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `role` | `string` | Role of the message. The available roles depend on the backend model being used, please consult the documentation for more information. |
| `content` | `string` | String content of the message. |




---
### AIPromptGuard

 
Settings for the Prompt Guard feature

```yaml
"request": .ai.options.gloo.solo.io.AIPromptGuard.Request
"response": .ai.options.gloo.solo.io.AIPromptGuard.Response

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `request` | [.ai.options.gloo.solo.io.AIPromptGuard.Request](../ai.proto.sk/#request) | Guards for the prompt request. |
| `response` | [.ai.options.gloo.solo.io.AIPromptGuard.Response](../ai.proto.sk/#response) | Guards for the LLM response. |




---
### Regex

 
Regex settings for prompt guard

```yaml
"matches": []ai.options.gloo.solo.io.AIPromptGuard.Regex.RegexMatch
"builtins": []ai.options.gloo.solo.io.AIPromptGuard.Regex.BuiltIn
"action": .ai.options.gloo.solo.io.AIPromptGuard.Regex.Action

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `matches` | [[]ai.options.gloo.solo.io.AIPromptGuard.Regex.RegexMatch](../ai.proto.sk/#regexmatch) | A list of Regex patterns to match against the response. All matches will be masked before being sent back to the client. matches and builtins are additive. |
| `builtins` | [[]ai.options.gloo.solo.io.AIPromptGuard.Regex.BuiltIn](../ai.proto.sk/#builtin) | A list of built-in regexes to mask in the response. matches and builtins are additive. |
| `action` | [.ai.options.gloo.solo.io.AIPromptGuard.Regex.Action](../ai.proto.sk/#action) | The action to take if the regex matches NOTE: This will only apply to request matches, response matches will always mask. |




---
### RegexMatch



```yaml
"pattern": string
"name": string

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `pattern` | `string` | The regex pattern to match against the response. |
| `name` | `string` | An optional name for this match which can be used for debugging purposes. |




---
### BuiltIn



| Name | Description |
| ----- | ----------- | 
| `SSN` | Default REGEX for Social Security Numbers |
| `CREDIT_CARD` | Default REGEX for Credit Card Numbers |
| `PHONE_NUMBER` | Default REGEX for Phone Numbers |
| `EMAIL` | Default REGEX for Email Addresses |




---
### Action



| Name | Description |
| ----- | ----------- | 
| `MASK` | Mask the response if the regex matches |
| `REJECT` | Reject the request if the regex matches |




---
### Webhook

 
Webhook settings for prompt guard

```yaml
"host": string
"port": int
"forwardHeaders": []ai.options.gloo.solo.io.AIPromptGuard.Webhook.HeaderMatch

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `host` | `string` | Host to send the traffic to. |
| `port` | `int` | Port to send the traffic to. |
| `forwardHeaders` | [[]ai.options.gloo.solo.io.AIPromptGuard.Webhook.HeaderMatch](../ai.proto.sk/#headermatch) | Headers to forward with the request. |




---
### HeaderMatch



```yaml
"key": string
"matchType": .ai.options.gloo.solo.io.AIPromptGuard.Webhook.HeaderMatch.MatchType

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `key` | `string` | Header key to match. |
| `matchType` | [.ai.options.gloo.solo.io.AIPromptGuard.Webhook.HeaderMatch.MatchType](../ai.proto.sk/#matchtype) | Type of match to use. |




---
### MatchType



| Name | Description |
| ----- | ----------- | 
| `EXACT` | Exact match |
| `PREFIX` | Prefix match |
| `SUFFIX` | Suffix match |
| `CONTAINS` | Contains match |
| `REGEX` | Regex match |




---
### Moderation



```yaml
"openai": .ai.options.gloo.solo.io.AIPromptGuard.Moderation.OpenAI

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `openai` | [.ai.options.gloo.solo.io.AIPromptGuard.Moderation.OpenAI](../ai.proto.sk/#openai) | OpenAI moderation. |




---
### OpenAI

 
OpenAI Moderation

```yaml
"model": string
"authToken": .ai.options.gloo.solo.io.SingleAuthToken

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `model` | `string` | The name of the moderation model to use, will default to: `omni-moderation-latest`. |
| `authToken` | [.ai.options.gloo.solo.io.SingleAuthToken](../ai.proto.sk/#singleauthtoken) |  |




---
### Request

 
Request settings for Prompt Guard

```yaml
"customResponse": .ai.options.gloo.solo.io.AIPromptGuard.Request.CustomResponse
"regex": .ai.options.gloo.solo.io.AIPromptGuard.Regex
"webhook": .ai.options.gloo.solo.io.AIPromptGuard.Webhook
"moderation": .ai.options.gloo.solo.io.AIPromptGuard.Moderation

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `customResponse` | [.ai.options.gloo.solo.io.AIPromptGuard.Request.CustomResponse](../ai.proto.sk/#customresponse) | Custom response message to send back to the client. If not specified, the following default message will be used: "The request was rejected due to inappropriate content". |
| `regex` | [.ai.options.gloo.solo.io.AIPromptGuard.Regex](../ai.proto.sk/#regex) | Regex request guard. |
| `webhook` | [.ai.options.gloo.solo.io.AIPromptGuard.Webhook](../ai.proto.sk/#webhook) | Webhook request guard. |
| `moderation` | [.ai.options.gloo.solo.io.AIPromptGuard.Moderation](../ai.proto.sk/#moderation) | Moderation settings. |




---
### CustomResponse



```yaml
"message": string
"statusCode": int

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `message` | `string` | Custom response message to send back to the client. If not specified, the following default message will be used: "The request was rejected due to inappropriate content". |
| `statusCode` | `int` | Status code to send back to the client. |




---
### Response

 
Request settings for Prompt Guard

```yaml
"regex": .ai.options.gloo.solo.io.AIPromptGuard.Regex
"webhook": .ai.options.gloo.solo.io.AIPromptGuard.Webhook

```

| Field | Type | Description |
| ----- | ---- | ----------- | 
| `regex` | [.ai.options.gloo.solo.io.AIPromptGuard.Regex](../ai.proto.sk/#regex) | Regex response guard. |
| `webhook` | [.ai.options.gloo.solo.io.AIPromptGuard.Webhook](../ai.proto.sk/#webhook) | Webhook response guard. |





<!-- Start of HubSpot Embed Code -->
<script type="text/javascript" id="hs-script-loader" async defer src="//js.hs-scripts.com/5130874.js"></script>
<!-- End of HubSpot Embed Code -->
