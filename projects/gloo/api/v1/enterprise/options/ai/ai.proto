syntax = "proto3";
package ai.options.gloo.solo.io;
option go_package = "github.com/solo-io/gloo/projects/gloo/pkg/api/v1/enterprise/options/ai";

import "github.com/solo-io/solo-kit/api/v1/ref.proto";
import "google/protobuf/struct.proto";
import "extproto/ext.proto";
option (extproto.equal_all) = true;
option (extproto.hash_all) = true;
option (extproto.clone_all) = true;

// The authorization token that the AI gateway uses to access the LLM provider API.
// This token is automatically sent in a request header, depending on the LLM provider.
message SingleAuthToken {

  // Configuration for passthrough of the existing token.
  // Currently, specifying an empty object (`passthrough: {}`)
  // indicates that passthrough will be used for auth.
  message Passthrough {
    // Use a message to allow for future expansion
  }

  oneof auth_token_source {
    // Provide the token directly in the configuration for the Upstream.
    // This option is the least secure. Only use this option for quick tests such as trying out AI Gateway.
    string inline = 1;
    // Store the API key in a Kubernetes secret in the same namespace as the Upstream.
    // Then, refer to the secret in the Upstream configuration. This option is more secure than an inline token,
    // because the API key is encoded and you can restrict access to secrets through RBAC rules.
    // You might use this option in proofs of concept, controlled development and staging environments,
    // or well-controlled prod environments that use secrets.
    core.solo.io.ResourceRef secret_ref = 2;
    // Passthrough the existing token. This token can either 
    // come directly from the client, or be generated by an OIDC flow
    // early in the request lifecycle. This option is useful for 
    // backends which have federated identity setup and can re-use
    // the token from the client.
    // Currently, this token must exist in the `Authorization` header.
    Passthrough passthrough = 3;
  }
}

// When you deploy the Gloo AI Gateway, you can use the `spec.ai` section of the Upstream resource
// to represent a backend for a logical Large Language Model (LLM) provider.
// This section configures the LLM provider that the AI Gateway routes requests to,
// and how the gateway should authenticate with the provider.
// Note that other Gloo AI Gateway LLM features, such as prompt guards
// and prompt enrichment, are configured at the route level in the
// [`spec.options.ai` section of the RouteOptions resource](#routesettings).
//
// To get started, see [About Gloo AI Gateway]({{% versioned_link_path fromRoot="/ai/overview/" %}}).
// For more information about the Upstream resource, see the
// [API reference]({{% versioned_link_path fromRoot="/reference/api/github.com/solo-io/gloo/projects/gloo/api/v1/upstream.proto.sk/" %}}).
//
// {{% notice note %}}
// AI Gateway is an Enterprise-only feature that requires a Gloo Gateway Enterprise license with an AI Gateway add-on.
// {{% /notice %}}
message UpstreamSpec {

  // Send requests to a custom host and port, such as to proxy the request,
  // or to use a different backend that is API-compliant with the upstream version.
  message CustomHost {
    // Custom host to send the traffic requests to.
    string host = 1;
    // Custom port to send the traffic requests to.
    uint32 port = 2;
  }

  // Settings for the [OpenAI](https://platform.openai.com/docs/overview) LLM provider.
  message OpenAI {
    // The authorization token that the AI gateway uses to access the OpenAI API.
    // This token is automatically sent in the `Authorization` header of the
    // request and prefixed with `Bearer`.
    SingleAuthToken auth_token = 1;
    // Optional: Send requests to a custom host and port, such as to proxy the request,
    // or to use a different backend that is API-compliant with the upstream version.
    CustomHost custom_host = 2;
    // Optional: Override the model name, such as `gpt-4o-mini`.
    // If unset, the model name is taken from the request.
    // This setting can be useful when setting up model failover within the same LLM provider.
    string model = 3;
  }

  // Settings for the [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/) LLM provider.
  // To find the values for the endpoint, deployment name, and API version, you can check the fields of an API request, such as
  // `https://{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}`.
  message AzureOpenAI {
    // The authorization token that the AI gateway uses to access the Azure OpenAI API.
    // This token is automatically sent in the `api-key` header of the request.
    oneof auth_token_source {
      // The authorization token that the AI gateway uses to access the Azure OpenAI API.
      // This token is automatically sent in the `api-key` header of the request.
      SingleAuthToken auth_token = 1;
      // use AD or other workload identity mechanism
    }

    // The endpoint for the Azure OpenAI API to use, such as `my-endpoint.openai.azure.com`.
    // If the scheme is included, it is stripped.
    string endpoint = 2;
    // The name of the Azure OpenAI model deployment to use.
    // For more information, see the [Azure OpenAI model docs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models).
    string deployment_name = 3;
    // The version of the Azure OpenAI API to use.
    // For more information, see the [Azure OpenAI API version reference](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#api-specs).
    string api_version = 4;
  }

  // Settings for the [Gemini](https://ai.google.dev/gemini-api/docs) LLM provider.
  // To find the values for the model and API version, you can check the fields of an API request, such as
  // `https://generativelanguage.googleapis.com/{version}/models/{model}:generateContent?key={api_key}`.
  message Gemini {
    // The authorization token that the AI gateway uses to access the Gemini API.
    // This token is automatically sent in the `key` query parameter of the request.
    oneof auth_token_source {
      // The authorization token that the AI gateway uses to access the Gemini API.
      // This token is automatically sent in the `key` query parameter of the request.
      SingleAuthToken auth_token = 1;
      // TODO: use oauth
    }

    // The Gemini model to use.
    // For more information, see the [Gemini models docs](https://ai.google.dev/gemini-api/docs/models/gemini).
    string model = 2;
    // The version of the Gemini API to use.
    // For more information, see the [Gemini API version docs](https://ai.google.dev/gemini-api/docs/api-versions).
    string api_version = 3;
  }

  // Settings for the [Vertex AI](https://cloud.google.com/vertex-ai/docs) LLM provider.
  // To find the values for the project ID, project location, and publisher, you can check the fields of an API request, such as
  // `https://{LOCATION}-aiplatform.googleapis.com/{VERSION}/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/{PROVIDER}/<model-path>`.
  message VertexAI {
    // The authorization token that the AI gateway uses to access the Vertex AI API.
    // This token is automatically sent in the `key` header of the request.
    oneof auth_token_source {
      // The authorization token that the AI gateway uses to access the Vertex AI API.
      // This token is automatically sent in the `key` header of the request.
      SingleAuthToken auth_token = 1;
      // TODO: use oauth
    }

    // The Vertex AI model to use.
    // For more information, see the [Vertex AI model docs](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models).
    string model = 2;
    // The version of the Vertex AI API to use.
    // For more information, see the [Vertex AI API reference](https://cloud.google.com/vertex-ai/docs/reference#versions).
    string api_version = 3;
    // The ID of the Google Cloud Project that you use for the Vertex AI.
    string project_id = 4;
    // The location of the Google Cloud Project that you use for the Vertex AI.
    string location = 5;
    // Optional: The model path to route to. Defaults to the Gemini model path, `generateContent`.
    string model_path = 6;

    // The type of publisher model to use. Currently, only Google is supported.
    enum Publisher {
      GOOGLE = 0;
    }

    // The type of publisher model to use. Currently, only Google is supported.
    Publisher publisher = 7;
  }

  // Settings for the [Mistral AI](https://docs.mistral.ai/getting-started/quickstart/) LLM provider.
  message Mistral {
    // The authorization token that the AI gateway uses to access the OpenAI API.
    // This token is automatically sent in the `Authorization` header of the
    // request and prefixed with `Bearer`.
    SingleAuthToken auth_token = 1;
    // Optional: Send requests to a custom host and port, such as to proxy the request,
    // or to use a different backend that is API-compliant with the upstream version.
    CustomHost custom_host = 2;
    // Optional: Override the model name.
    // If unset, the model name is taken from the request.
    // This setting can be useful when testing model failover scenarios.
    string model = 3;
  }

  // Settings for the [Anthropic](https://docs.anthropic.com/en/release-notes/api) LLM provider.
  message Anthropic {
    // The authorization token that the AI gateway uses to access the Anthropic API.
    // This token is automatically sent in the `x-api-key` header of the request.
    SingleAuthToken auth_token = 1;
    // Optional: Send requests to a custom host and port, such as to proxy the request,
    // or to use a different backend that is API-compliant with the upstream version.
    CustomHost custom_host = 2;
    // Optional: A version header to pass to the Anthropic API.
    // For more information, see the [Anthropic API versioning docs](https://docs.anthropic.com/en/api/versioning).
    string version = 3;
    // Optional: Override the model name.
    // If unset, the model name is taken from the request.
    // This setting can be useful when testing model failover scenarios.
    string model = 4;
  }

  // Configure backends for multiple hosts or models from the same provider in one Upstream resource.
  // This method can be useful for creating one logical endpoint that is backed
  // by multiple hosts or models.
  //
  // In the `priorities` section, the order of `pool` entries defines the priority of the backend endpoints.
  // The `pool` entries can either define a list of backends or a single backend.
  // Note: Only two levels of nesting are permitted. Any nested entries after the second level are ignored.
  //
  // ```yaml
  // multi:
  //   priorities:
  //   - pool:
  //     - azureOpenai:
  //         deploymentName: gpt-4o-mini
  //         apiVersion: 2024-02-15-preview
  //         endpoint: ai-gateway.openai.azure.com
  //         authToken:
  //           secretRef:
  //             name: azure-secret
  //             namespace: gloo-system
  //   - pool:
  //     - azureOpenai:
  //         deploymentName: gpt-4o-mini-2
  //         apiVersion: 2024-02-15-preview
  //         endpoint: ai-gateway-2.openai.azure.com
  //         authToken:
  //           secretRef:
  //             name: azure-secret-2
  //             namespace: gloo-system
  // ```
  message MultiPool {
    // An entry represeting an LLM provider backend that the AI Gateway routes requests to.
    message Backend {
      oneof llm {
        // Configure an [OpenAI](https://platform.openai.com/docs/overview) backend.
        OpenAI openai = 1;
        // Configure a [Mistral AI](https://docs.mistral.ai/getting-started/quickstart/) backend.
        Mistral mistral = 2;
        // Configure an [Anthropic](https://docs.anthropic.com/en/release-notes/api) backend.
        Anthropic anthropic = 3;
        // Configure an [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/) backend.
        AzureOpenAI azure_openai = 4;
        // Configure a [Gemini](https://ai.google.dev/gemini-api/docs) backend.
        Gemini gemini = 5;
        // Configure a [Vertex AI](https://cloud.google.com/vertex-ai/docs) backend.
        VertexAI vertex_ai = 6;
      }
    }

    // The order of `pool` entries within this section defines the priority of the backend endpoints.
    message Priority {
      // A list of LLM provider backends within a single endpoint pool entry.
      repeated Backend pool = 1;
    }

    // The order of `pool` entries within this section defines the priority of the backend endpoints.
    repeated Priority priorities = 1;
  }

  oneof llm {
    // Configure an [OpenAI](https://platform.openai.com/docs/overview) backend.
    OpenAI openai = 1;
    // Configure a [Mistral AI](https://docs.mistral.ai/getting-started/quickstart/) backend.
    Mistral mistral = 2;
    // Configure an [Anthropic](https://docs.anthropic.com/en/release-notes/api) backend.
    Anthropic anthropic = 3;
    // Configure an [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/) backend.
    AzureOpenAI azure_openai = 4;
    // Configure backends for multiple LLM providers in one logical endpoint.
    MultiPool multi = 5;
    // Configure a [Gemini](https://ai.google.dev/gemini-api/docs) backend.
    Gemini gemini = 6;
    // Configure a [Vertex AI](https://cloud.google.com/vertex-ai/docs) backend.
    VertexAI vertex_ai = 7;
  }
}

// When you deploy the Gloo AI Gateway, you can use the `spec.options.ai` section
// of the RouteOptions resource to configure the behavior of the LLM provider
// on the level of individual routes. These route settings, such as prompt enrichment,
// retrieval augmented generation (RAG), and semantic caching, are applicable only
// for routes that send requests to an LLM provider backend.
//
// For more information about the RouteOptions resource, see the
// [API reference]({{% versioned_link_path fromRoot="/reference/api/github.com/solo-io/gloo/projects/gloo/api/v1/route_options.proto.sk/" %}}).
message RouteSettings {

  // Enrich requests sent to the LLM provider by appending and prepending system prompts.
  // This can be configured only for LLM providers that use the `CHAT` API route type.
  AIPromptEnrichment prompt_enrichment = 1;

  // Set up prompt guards to block unwanted requests to the LLM provider and mask sensitive data.
  // Prompt guards can be used to reject requests based on the content of the prompt, as well as
  // mask responses based on the content of the response.
  AIPromptGuard prompt_guard = 2;

  // [Retrieval augmented generation (RAG)](https://research.ibm.com/blog/retrieval-augmented-generation-RAG)
  // is a technique of providing relevant context by retrieving relevant data from one or more
  // context datasets and augmenting the prompt with the retrieved information.
  // This can be used to improve the quality of the generated text.
  RAG rag = 3;

  // Cache previous model responses to provide faster responses to similar requests in the future.
  // Results might vary depending on the embedding mechanism used, as well
  // as the similarity threshold set.
  SemanticCache semantic_cache = 4;

  // Provide defaults to merge with user input fields.
  // Defaults do _not_ override the user input fields, unless you explicitly set `override` to `true`.
  repeated FieldDefault defaults = 5;

  // The type of route to the LLM provider API.
  enum RouteType {
    // The LLM generates the full response before responding to a client.
    CHAT = 0;
    // Stream responses to a client, which allows the LLM to stream out tokens as they are generated. 
    CHAT_STREAMING = 1;
  }

  // The type of route to the LLM provider API. Currently, `CHAT` and `CHAT_STREAMING` are supported.
  RouteType route_type = 6;
}

// Provide defaults to merge with user input fields.
// Defaults do _not_ override the user input fields, unless you explicitly set `override` to `true`.
//
// Example overriding the system field for Anthropic:
// ```yaml
// # Anthropic doesn't support a system chat type
// defaults:
// - field: "system"
//   value: "answer all questions in french"
// ```
//
// Example setting the temperature and overriding `max_tokens`:
// ```yaml
// defaults:
// - field: "temperature"
//   value: 0.5
// - field: "max_tokens"
//   value: 100
// ```
message FieldDefault {
  // The name of the field.
  string field = 1;
  // The field default value, which can be any JSON Data Type.
  google.protobuf.Value value = 2;
  // Whether to override the field's value if it already exists.
  // Defaults to false.
  bool override = 3;
}

// Configuration settings for a Postgres datastore.
message Postgres {
  // Connection string to the Postgres database. For example, to use a vector database
  // deployed to your cluster, your connection string might look similar to
  // `postgresql+psycopg://gloo:gloo@vector-db.default.svc.cluster.local:5432/gloo`.
  string connection_string = 1;
  // Name of the collection table to use.
  string collection_name = 2;
}

// Configuration of the API used to generate the embedding.
message Embedding {

  // Embedding settings for the OpenAI provider.
  message OpenAI {
    oneof auth_token_source {
      // The authorization token that the AI gateway uses to access the OpenAI API.
      // This token is automatically sent in the `Authorization` header of the
      // request and prefixed with `Bearer`.
      SingleAuthToken auth_token = 1;
      // re-use the token from the backend
      // google.protobuf.Empty inherit_backend_token = 3;
    }
  }

  // Embedding settings for the Azure OpenAI provider.
  message AzureOpenAI {
    oneof auth_token_source {
      // The authorization token that the AI gateway uses to access the Azure OpenAI API.
      // This token is automatically sent in the `api-key` header of the request.
      SingleAuthToken auth_token = 1;
      // re-use the token from the backend
      // google.protobuf.Empty inherit_backend_token = 3;
    }

    // The version of the Azure OpenAI API to use.
    // For more information, see the [Azure OpenAI API version reference](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#api-specs).
    string api_version = 2;
    // The endpoint for the Azure OpenAI API to use, such as `my-endpoint.openai.azure.com`.
    // If the scheme is not included, it is added.
    string endpoint = 3;
    // The name of the Azure OpenAI model deployment to use.
    // For more information, see the [Azure OpenAI model docs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models).
    string deployment_name = 4;
  }

  // Configuration for the backend LLM provider authentication token.
  oneof embedding {
    // Embedding settings for the OpenAI provider.
    OpenAI openai = 1;
    // Embedding settings for the Azure OpenAI provider.
    AzureOpenAI azure_openai = 2;
  }
}

// Cache previous model responses to provide faster responses to similar requests in the future.
// Results might vary depending on the embedding mechanism used, as well
// as the similarity threshold set. Semantic caching reduces the number of requests
// to the LLM provider, improves the response time, and reduces costs.
//
// Example configuring a route to use a `redis` datastore and OpenAI for RAG:
// ```yaml
// semanticCache:
//   datastore:
//     redis:
//       connectionString: redis://172.17.0.1:6379
//   embedding:
//     openai:
//       authToken:
//         secretRef:
//           name: openai-secret
//           namespace: gloo-system
// ```
message SemanticCache {

  // Settings for a Redis database.
  message Redis {
    // Connection string to the Redis database, such as `redis://172.17.0.1:6379`.
    string connection_string = 1;

    // Similarity score threshold value between 0.0 and 1.0 that determines how similar
    // two queries must be in order to return a cached result.
    // The lower the number, the more similar the queries must be for a cache hit.
    //
    // +kubebuilder:validation:Minimum=0
    // +kubebuilder:validation:Maximum=1
    float score_threshold = 2;
  }

  // Settings for a Weaviate database.
  message Weaviate {
    // Connection string to the Weaviate database.
    // Do not include the scheme. For example, the format
    // `weaviate.my-ns.svc.cluster.local` is correct. The format
    // `http://weaviate.my-ns.svc.cluster.local`, which includes the scheme, is incorrect.
    string host = 1;
    // HTTP port to use. If unset, defaults to `8080`.
    uint32 http_port = 2;
    // GRPC port to use. If unset, defaults to `50051`.
    uint32 grpc_port = 3;
    // Whether to use a secure connection. Defaults to `true`.
    bool insecure = 4;
  }

  // Data store from which to cache the request and response pairs.
  message DataStore {
    oneof datastore {
      // Settings for a Redis database.
      Redis redis = 1;
      // Settings for a Weaviate database.
      Weaviate weaviate = 2;
    }
  }
  // The caching mode to use for the request and response lifecycle.
  enum Mode {
    // Read and write to the cache as a part of the request and response lifecycle.
    READ_WRITE = 0;
    // Only read from the cache, and do not write to it.
    // Data is written to the cache outside of the request and response cycle.
    READ_ONLY = 1;
  }
  // Data store from which to cache the request and response pairs.
  DataStore datastore = 1;
  // Model to use to retrieve the embedding mechanism.
  Embedding embedding = 2;
  // Time before data in the cache is considered expired.
  uint32 ttl = 3;
  // The caching mode to use for the request and response lifecycle. Supported values include `READ_WRITE` or `READ_ONLY`.
  Mode mode = 4;
}

// [Retrieval augmented generation (RAG)](https://research.ibm.com/blog/retrieval-augmented-generation-RAG)
// is a technique of providing relevant context by retrieving relevant data from one or more
// context datasets and augmenting the prompt with the retrieved information.
// This can be used to improve the quality of the generated text.
//
// {{% notice note %}}
// The same embedding mechanism that was used for the initial
// creation of the context datasets must be used for the prompt.
// {{% /notice %}}
//
// Example configuring a route to use a `postgres` datastore and OpenAI for RAG:
// ```yaml
// rag:
//   datastore:
//     postgres:
//       connectionString: postgresql+psycopg://gloo:gloo@172.17.0.1:6024/gloo
//       collectionName: default
//   embedding:
//     openai:
//       authToken:
//         secretRef:
//           name: openai-secret
//           namespace: gloo-system
// ```
//
// {{% notice tip %}}
// For an extended example that includes deploying a vector database with a context dataset,
// check out the [Retrieval augmented generation (RAG) tutorial](https://docs.solo.io/gateway/main/ai/tutorials/rag/).
// {{% /notice %}}
message RAG {
  message DataStore {
    oneof datastore {
      // Configuration settings for a Postgres datastore.
      Postgres postgres = 1;
    }
  }
  // Data store from which to fetch the context embeddings.
  DataStore datastore = 1;
  // Model to use to retrieve the context embeddings.
  Embedding embedding = 2;
  // Template to use to embed the returned context.
  string prompt_template = 3;
}

// Enrich requests sent to the LLM provider by appending and prepending system prompts.
// This can be configured only for LLM providers that use the CHAT API type.
//
// Prompt enrichment allows you to add additional context to the prompt before sending it to the model.
// Unlike RAG or other dynamic context methods, prompt enrichment is static and is applied to every request.
//
// **Note**: Some providers, including Anthropic, do not support SYSTEM role messages, and instead have a dedicated
// system field in the input JSON. In this case, use the [`defaults` setting](#fielddefault) to set the system field.
//
// The following example prepends a system prompt of `Answer all questions in French.`
// and appends `Describe the painting as if you were a famous art critic from the 17th century.`
// to each request that is sent to the `openai` HTTPRoute.
// ```yaml
// apiVersion: gateway.solo.io/v1
// kind: RouteOption
// metadata:
//   name: openai-opt
//   namespace: gloo-system
// spec:
//   targetRefs:
//   - group: gateway.networking.k8s.io
//     kind: HTTPRoute
//     name: openai
//   options:
//     ai:
//       promptEnrichment:
//         prepend:
//         - role: SYSTEM
//           content: "Answer all questions in French."
//         append:
//         - role: USER
//           content: "Describe the painting as if you were a famous art critic from the 17th century."
// ```
message AIPromptEnrichment {
  // An entry for a message to prepend or append to each prompt.
  message Message {
    // Role of the message. The available roles depend on the backend
    // LLM provider model, such as `SYSTEM` or `USER` in the OpenAI API.
    string role = 1;
    // String content of the message.
    string content = 2;
  }
  // A list of messages to be prepended to the prompt sent by the client.
  repeated Message prepend = 2;
  // A list of messages to be appended to the prompt sent by the client.
  repeated Message append = 3;
}

// Set up prompt guards to block unwanted requests to the LLM provider and mask sensitive data.
// Prompt guards can be used to reject requests based on the content of the prompt, as well as
// mask responses based on the content of the response.
//
// This example rejects any request prompts that contain
// the string "credit card", and masks any credit card numbers in the response.
// ```yaml
// promptGuard:
//   request:
//     customResponse: 
//       message: "Rejected due to inappropriate content"
//     regex:
//       action: REJECT
//       matches:
//       - pattern: "credit card"
//         name: "CC"
//   response:
//     regex:
//       builtins:
//       - CREDIT_CARD
//       action: MASK
// ```
message AIPromptGuard {

  // Regular expression (regex) matching for prompt guards and data masking.
  message Regex {
    // Built-in regex patterns for specific types of strings in prompts.
    // For example, if you specify `CREDIT_CARD`, any credit card numbers
    // in the request or response are matched.
    enum BuiltIn {
      // Default regex matching for Social Security numbers.
      SSN = 0;
      // Default regex matching for credit card numbers.
      CREDIT_CARD = 1;
      // Default regex matching for phone numbers.
      PHONE_NUMBER = 2;
      // Default regex matching for email addresses.
      EMAIL = 3;
    }

    // Regular expression (regex) matching for prompt guards and data masking.
    message RegexMatch {
      // The regex pattern to match against the request or response.
      string pattern = 1;
      // An optional name for this match, which can be used for debugging purposes.
      string name = 2;
    }
    // A list of regex patterns to match against the request or response.
    // Matches and built-ins are additive.
    repeated RegexMatch matches = 1;
    // A list of built-in regex patterns to match against the request or response.
    // Matches and built-ins are additive.
    repeated BuiltIn builtins = 2;

    // The action to take if a regex pattern is matched in a request or response.
    // This setting applies only to request matches. Response matches are always masked by default.
    enum Action {
      // Mask the matched data in the request.
      MASK = 0;
      // Reject the request if the regex matches content in the request.
      REJECT = 1;
    }

    // The action to take if a regex pattern is matched in a request or response.
    // This setting applies only to request matches. Response matches are always masked by default.
    Action action = 3;
  }

  // Configure a webhook to forward requests or responses to for prompt guarding.
  message Webhook {
    // Host to send the traffic to.
    string host = 1;
    // Port to send the traffic to
    uint32 port = 2;
    // Describes how to match a given string in HTTP headers. Match is case-sensitive.
    message HeaderMatch {
      // The header string match type.
      enum MatchType {
        // The string must match exactly the specified string.
        EXACT = 0;
        // The string must have the specified prefix.
        PREFIX = 1;
        // The string must have the specified suffix.
        SUFFIX = 2;
        // The header string must contain the specified string.
        CONTAINS = 3;
        // The string must match the specified [RE2-style regular expression](https://github.com/google/re2/wiki/) pattern.
        regex = 4;
      }
      // The header key string to match against.
      string key = 1;
      // The type of match to use.
      MatchType match_type = 2;
    }
    // Headers to forward with the request to the webhook.
    repeated HeaderMatch forwardHeaders = 3;
  }

  // Pass prompt data through an external moderation model endpoint,
  // which compares the request prompt input to predefined content rules.
  // Any requests that are routed through Gloo AI Gateway pass through the
  // moderation model that you specify. If the content is identified as harmful
  // according to the model's content rules, the request is automatically rejected.
  //
  // You can configure an moderation endpoint either as a standalone prompt guard setting
  // or in addition to other request and response guard settings.
  message Moderation {
      // Configure an OpenAI moderation endpoint.
      message OpenAI {
        // The name of the OpenAI moderation model to use. Defaults to
        // [`omni-moderation-latest`](https://platform.openai.com/docs/guides/moderation).
        string model = 1;

        // The authorization token that the AI gateway uses
        // to access the OpenAI moderation model.
        oneof auth_token_source {
          // The authorization token that the AI gateway uses
          // to access the OpenAI moderation model.
          SingleAuthToken auth_token = 2;
          // re-use the token from the backend
          // google.protobuf.Empty inherit_backend_token = 3;
        }
      }

      // Pass prompt data through an external moderation model endpoint,
      // which compares the request prompt input to predefined content rules.
      oneof moderation {
        // Configure an OpenAI moderation endpoint.
        OpenAI openai = 1;
      }
  }

  // Prompt guards to apply to requests sent by the client.
  message Request {
    // A custom response to return to the client if request content
    // is matched against a regex pattern and the action is `REJECT`.
    message CustomResponse {
      // A custom response message to return to the client. If not specified, defaults to
      // "The request was rejected due to inappropriate content".
      string message = 1;

      // The status code to return to the client.
      uint32 status_code = 2;
    }
    // A custom response message to return to the client. If not specified, defaults to
    // "The request was rejected due to inappropriate content".
    CustomResponse custom_response = 1;

    // Regular expression (regex) matching for prompt guards and data masking.
    Regex regex = 2;

    // Configure a webhook to forward requests to for prompt guarding.
    Webhook webhook = 3;

    // Pass prompt data through an external moderation model endpoint,
    // which compares the request prompt input to predefined content rules.
    Moderation moderation = 4;
  }

  // Prompt guards to apply to responses returned by the LLM provider.
  message Response {
    // Regular expression (regex) matching for prompt guards and data masking.
    Regex regex = 1;

    // Configure a webhook to forward responses to for prompt guarding.
    Webhook webhook = 2;
  }
  // Prompt guards to apply to requests sent by the client.
  Request request = 1;
  // Prompt guards to apply to responses returned by the LLM provider.
  Response response = 2;
}